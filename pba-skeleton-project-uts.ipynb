{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project UTS - Pengolahan Bahasa Alami\n",
    "##  KLASIFIKASI TEKS BAHASA INDONESIA UNTUK ...\n",
    "\n",
    "**Anggota Kelompok:**\n",
    "1. Stefanus Vemas Aditya Mahardika - 210711398\n",
    "2. Alexis Divasonda Sigat Ngaing - 210711407\n",
    "3. Dave Sebastian Petrus - 210711487\n",
    "4. Tio Pramudya - 210711411"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "Silahkan isi bagian ini dengan dengan objektif/tujuan dari ide projek kelompok Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latar Belakang\n",
    "Silahkan isi bagian ini dengan dengan latar belakang dari ide projek kelompok Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deskripsi Dataset\n",
    "Silahkan isi bagian ini dengan sumber dataset untuk projek kelompok Anda, bagaimana kelompok Anda mengumpulkan data, dan karakteristik dari dataset kelompok Anda, misal jumlah untuk masing-masing label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pengembangan Model Klasifikasi Teks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inisialisasi\n",
    "Silakan isi bagian ini dengan mengimport library-library yang akan digunakan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from contractions_id import CONTRACTION_MAP\n",
    "import modSpellChecker as sc\n",
    "# from contractions_id import CONTRACTION_MAP_ID\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import models\n",
    "#from pattern.en import tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from gensim import corpora, models\n",
    "import sklearn\n",
    "#from normalization import normalize_corpus\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Silakan isi bagian ini dengan script untuk memanggil data yang akan digunakan untuk keperluan pengembangan model klasifikasi teks dan menampilkan contoh data yang telah dipanggil baik yang akan dijadikan sebagai input maupun target-nya (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melisa Aritonang</td>\n",
       "      <td>Masih berada di kaki gunung\\nSeru sekali bisa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Irsya Kumala</td>\n",
       "      <td>Amazing ..\\nSangat Suka,,. pertama kali kesini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dewi Natalia</td>\n",
       "      <td>Ke merapi naik jeep  tahun 2016 seru banget ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fajar Aqil</td>\n",
       "      <td>Banyak pemandangan sangat indah yang bisa dile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wawan budiana wikanda</td>\n",
       "      <td>Sebuah pengalaman yg tak akan terulang 2x\\nPen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Hiren Patel</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Reynaldi pramudya</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Douglas Lee-OLoughlin</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Retno Tri Utami</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Bonifasius Rafandra</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                               text\n",
       "0         Melisa Aritonang  Masih berada di kaki gunung\\nSeru sekali bisa ...\n",
       "1             Irsya Kumala  Amazing ..\\nSangat Suka,,. pertama kali kesini...\n",
       "2             Dewi Natalia  Ke merapi naik jeep  tahun 2016 seru banget ka...\n",
       "3               Fajar Aqil  Banyak pemandangan sangat indah yang bisa dile...\n",
       "4    wawan budiana wikanda  Sebuah pengalaman yg tak akan terulang 2x\\nPen...\n",
       "..                     ...                                                ...\n",
       "995            Hiren Patel                                                NaN\n",
       "996      Reynaldi pramudya                                                NaN\n",
       "997  Douglas Lee-OLoughlin                                                NaN\n",
       "998        Retno Tri Utami                                                NaN\n",
       "999    Bonifasius Rafandra                                                NaN\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('merapi.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Melisa Aritonang</td>\n",
       "      <td>Masih berada di kaki gunung\\nSeru sekali bisa ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Irsya Kumala</td>\n",
       "      <td>Amazing ..\\nSangat Suka,,. pertama kali kesini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dewi Natalia</td>\n",
       "      <td>Ke merapi naik jeep  tahun 2016 seru banget ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fajar Aqil</td>\n",
       "      <td>Banyak pemandangan sangat indah yang bisa dile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wawan budiana wikanda</td>\n",
       "      <td>Sebuah pengalaman yg tak akan terulang 2x\\nPen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>901</th>\n",
       "      <td>Retno Aja</td>\n",
       "      <td>WAAAAWW ISMEZIK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>Bijoy Sorkar</td>\n",
       "      <td>Piculiar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>葉小良</td>\n",
       "      <td>Sering melutus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>Robbydwi Riski</td>\n",
       "      <td>Yuhu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Ilsuk Kim</td>\n",
       "      <td>활화산.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>880 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name                                               text\n",
       "0         Melisa Aritonang  Masih berada di kaki gunung\\nSeru sekali bisa ...\n",
       "1             Irsya Kumala  Amazing ..\\nSangat Suka,,. pertama kali kesini...\n",
       "2             Dewi Natalia  Ke merapi naik jeep  tahun 2016 seru banget ka...\n",
       "3               Fajar Aqil  Banyak pemandangan sangat indah yang bisa dile...\n",
       "4    wawan budiana wikanda  Sebuah pengalaman yg tak akan terulang 2x\\nPen...\n",
       "..                     ...                                                ...\n",
       "901              Retno Aja                                    WAAAAWW ISMEZIK\n",
       "902           Bijoy Sorkar                                        Piculiar...\n",
       "903                    葉小良                                     Sering melutus\n",
       "904         Robbydwi Riski                                               Yuhu\n",
       "905              Ilsuk Kim                                               활화산.\n",
       "\n",
       "[880 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = dataset.iloc[:,0]\n",
    "feature = dataset.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Melisa Aritonang\n",
       "1             Irsya Kumala\n",
       "2             Dewi Natalia\n",
       "3               Fajar Aqil\n",
       "4    wawan budiana wikanda\n",
       "5                mus takim\n",
       "6           joseph raharjo\n",
       "7             Hana Uswatun\n",
       "8       Yuliahan Nariswari\n",
       "9           Adinda Maulina\n",
       "Name: name, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Masih berada di kaki gunung\\nSeru sekali bisa ...\n",
       "1    Amazing ..\\nSangat Suka,,. pertama kali kesini...\n",
       "2    Ke merapi naik jeep  tahun 2016 seru banget ka...\n",
       "3    Banyak pemandangan sangat indah yang bisa dile...\n",
       "4    Sebuah pengalaman yg tak akan terulang 2x\\nPen...\n",
       "5    Gunung Merapi..\\nMerupakan Gunung teraktif did...\n",
       "6    Gunung merapi mencerminkan kebesaran Tuhan yan...\n",
       "7    Amazing banget, waktu pertama kali kesana udah...\n",
       "8    View pagi nya wlwpun berkabut bener bener bagu...\n",
       "9    My all time favorite volcano. A 5 star rating ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisasi Data\n",
    "Silakan isi bagian ini dengan script untuk keperluan normalisasi data teks yang akan digunakan untuk pengembangan model klasifikasi teks. \n",
    "Untuk setiap strategi normalisasi yang akan digunakan sebaiknya dibuat fungsi tersendiri dan dipanggil dalam satu fungsi normalisasi. Jangan lupa pada bagian ini ditampilkan contoh hasil normalisasi datanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "character = ['z','y','x','w','v','u','t','s','r','q','p','o','n','m','l','k','j','i','h','g','f','e','d',\n",
    "',','.',';',':','-','...','?','!','(',')','[',']','{','}','<','>','\"','/','\\'','#','-','@']\n",
    "\n",
    "def repeatcharNormalize(text):\n",
    "    for i in range(len(character)):\n",
    "        charac_long = 5\n",
    "        while charac_long>=2:\n",
    "            char=character[i]*charac_long\n",
    "            text=text.replace(char,character[i])\n",
    "            charac_long-=1\n",
    "        return text\n",
    "\n",
    "def spellNormalize(text):\n",
    "    spellCheck = []\n",
    "    for i in text:\n",
    "        if i not in character:\n",
    "            j=sc.correction(i)\n",
    "            spellCheck.append(j)\n",
    "        else:\n",
    "            spellCheck.append(i)\n",
    "        return spellCheck\n",
    "    \n",
    "def tokenize_text(text):\n",
    "    tokens=nltk.word_tokenize(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match) if contraction_mapping.get(match) else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "def stemmer_text(text):\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = stemmer.stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "factory = StopWordRemoverFactory()\n",
    "stopword_list = factory.get_stop_words()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus, tokenize=False):\n",
    "    normalized_corpus = []\n",
    "    for text in corpus:\n",
    "        text = expand_contractions(text, CONTRACTION_MAP)\n",
    "        text = stemmer_text(text)\n",
    "        text = remove_special_characters(text)\n",
    "        text = repeatcharNormalize(text)\n",
    "        text = remove_stopwords(text)\n",
    "        normalized_corpus.append(text)\n",
    "        if tokenize:\n",
    "            text = tokenize_text(text)\n",
    "            text = spellNormalize(text)\n",
    "            normalized_corpus.append(text)\n",
    "    return normalized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n",
    "    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=0.33,random_state=42)\n",
    "    return train_X, test_X, train_Y, test_Y\n",
    "\n",
    "train_corpus, test_corpus, train_labels, test_labels = prepare_datasets(feature,label,test_data_proportion=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_train_corpus = normalize_corpus(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sekali',\n",
       "  'ronde',\n",
       "  'kesini',\n",
       "  'yg',\n",
       "  'jogja',\n",
       "  'pandemi',\n",
       "  'nikmat',\n",
       "  'sudah',\n",
       "  'sapi'],\n",
       " ['gunung', 'at', 'aktif'],\n",
       " ['saya', 'cuma', 'sapi', 'cape', 'sinyal', 'yg', 'd', 'lunak'],\n",
       " ['marapi', 'at', 'mie', 'zapomnia', 'e', 'widoki'],\n",
       " ['mas',\n",
       "  'di',\n",
       "  'st',\n",
       "  'pic',\n",
       "  'ofrande',\n",
       "  'fa',\n",
       "  'tempat',\n",
       "  'st',\n",
       "  'cunosc',\n",
       "  'tehnicile',\n",
       "  'terapeutice',\n",
       "  'di',\n",
       "  'obiectele',\n",
       "  'cape',\n",
       "  'ke',\n",
       "  'beda',\n",
       "  'atinge',\n",
       "  'pentru',\n",
       "  'at',\n",
       "  'recupera',\n",
       "  'sate'],\n",
       " ['saya', 'sedikit'],\n",
       " ['yg',\n",
       "  'j',\n",
       "  'mau',\n",
       "  'i',\n",
       "  'i',\n",
       "  'gvjvuvunimininycivu',\n",
       "  'ynononyv',\n",
       "  'y',\n",
       "  'tvrct',\n",
       "  'yuk',\n",
       "  'uvtbu',\n",
       "  'by',\n",
       "  'h',\n",
       "  'y',\n",
       "  'by',\n",
       "  'ubyhhuh',\n",
       "  'hhjj',\n",
       "  'fa',\n",
       "  'h',\n",
       "  'hvuvg',\n",
       "  'yvuvggy',\n",
       "  'ubinibhnjbjbjbibukanij',\n",
       "  'di'],\n",
       " ['growing',\n",
       "  'at',\n",
       "  'watching',\n",
       "  'sapi',\n",
       "  'volcanic',\n",
       "  'mountain',\n",
       "  'spewing',\n",
       "  'ke',\n",
       "  'and',\n",
       "  'java',\n",
       "  'from',\n",
       "  'by',\n",
       "  'at',\n",
       "  'second',\n",
       "  'floor',\n",
       "  'balcony'],\n",
       " ['itu',\n",
       "  'aktif',\n",
       "  'suasana',\n",
       "  'candid',\n",
       "  'dari',\n",
       "  'at',\n",
       "  'tinggal',\n",
       "  'by',\n",
       "  'peristiwa',\n",
       "  'sudah',\n",
       "  'saya',\n",
       "  'at',\n",
       "  'syukur',\n",
       "  'yg',\n",
       "  'selamat'],\n",
       " ['sapi',\n",
       "  'naik',\n",
       "  'jeng',\n",
       "  '20',\n",
       "  'tanya',\n",
       "  'ayam',\n",
       "  'malam',\n",
       "  'hujan',\n",
       "  'gundul',\n",
       "  'murah',\n",
       "  'hancur']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_train_corpus[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_test_corpus = normalize_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['gunung',\n",
       "  'sapi',\n",
       "  'yg',\n",
       "  'gaya',\n",
       "  'pesona',\n",
       "  'yg',\n",
       "  'baru',\n",
       "  'naik',\n",
       "  'statesnya',\n",
       "  'siapa',\n",
       "  'mal',\n",
       "  'temen',\n",
       "  'baru',\n",
       "  'tantang',\n",
       "  'sendiri',\n",
       "  'wisatawan',\n",
       "  'yg',\n",
       "  'unjung'],\n",
       " ['tempat', 'saat', 'sudah', 'kesini'],\n",
       " ['apik', 'and'],\n",
       " ['gunung',\n",
       "  'rena',\n",
       "  'tempat',\n",
       "  'yg',\n",
       "  'sudah',\n",
       "  'sek',\n",
       "  'senang',\n",
       "  'daerah',\n",
       "  'istimewa',\n",
       "  'jogjakarta',\n",
       "  'putar',\n",
       "  'masyarakat',\n",
       "  'yg',\n",
       "  'adem',\n",
       "  'ayam',\n",
       "  'rame',\n",
       "  'pikuk',\n",
       "  'senang',\n",
       "  'wisata',\n",
       "  'bareng',\n",
       "  'gunung',\n",
       "  'sapi'],\n",
       " ['tempat',\n",
       "  'naik',\n",
       "  'lagi',\n",
       "  'dari',\n",
       "  'rutinitas',\n",
       "  'kerja',\n",
       "  'recharge',\n",
       "  'yuk',\n",
       "  'and',\n",
       "  'sore'],\n",
       " ['pikuk', 'e', 'tante', 'dan'],\n",
       " ['perlu', 'tanya', 'ayam', 'dari', 'sapi'],\n",
       " ['lagi',\n",
       "  'pasar',\n",
       "  'bareng',\n",
       "  'sapi',\n",
       "  'di',\n",
       "  'sekali',\n",
       "  'jangan',\n",
       "  'gunung',\n",
       "  'bisa',\n",
       "  'energi',\n",
       "  'rantau'],\n",
       " ['jeanpierre',\n",
       "  'baru',\n",
       "  'ke',\n",
       "  'promener',\n",
       "  'at',\n",
       "  'volcan',\n",
       "  'sapi',\n",
       "  'jogyakarta',\n",
       "  'jawa',\n",
       "  'indonesia',\n",
       "  'super'],\n",
       " ['jalan',\n",
       "  'sate',\n",
       "  'gunung',\n",
       "  'at',\n",
       "  'malang',\n",
       "  'aktif',\n",
       "  'indonesia',\n",
       "  'aku',\n",
       "  'gunung',\n",
       "  'senang',\n",
       "  'semua',\n",
       "  'lagi',\n",
       "  'terus',\n",
       "  'gembel',\n",
       "  'jawa',\n",
       "  'subur',\n",
       "  'harga',\n",
       "  'status',\n",
       "  'lagi',\n",
       "  'gunung',\n",
       "  'at',\n",
       "  'aktif',\n",
       "  'gak',\n",
       "  'jalan',\n",
       "  'lima',\n",
       "  'telur',\n",
       "  'erupsi',\n",
       "  'gunung',\n",
       "  'milik',\n",
       "  'tanya',\n",
       "  'jalan',\n",
       "  'dari',\n",
       "  'malang',\n",
       "  'lunak',\n",
       "  'lewat',\n",
       "  'kaliurang',\n",
       "  'tugu',\n",
       "  'tanya',\n",
       "  'bandung']]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_test_corpus[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstraksi Fitur\n",
    "Silakan isi bagian ini dengan script untuk keperluan ekstraksi fitur data teks yang akan digunakan untuk pengembangan model klasifikasi teks. \n",
    "Untuk setiap strategi ekstraksi fitur yang akan digunakan sebaiknya dibuat fungsi tersendiri dan dipanggil dalam satu fungsi ekstraksi fitur. Jangan lupa pada bagian ini ditampilkan contoh hasil ekstraksi fitur terhadap datanya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Silakan isi bagian ini dengan script untuk keperluan training model klasifikasi teks. Jangan lupa untuk menjabarkan rancangan pemodelan algoritme yang akan diterapkan dan kombinasi parameter yang akan digunakan dengan benar dan lengkap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Silakan isi bagian ini dengan script untuk keperluan evaluasi model klasifikasi teks. Jangan lupa untuk menjabarkan rancangan proses evaluasi model, metrik, dan visualisasi confusion matrix yang akan digunakan dengan benar dan lengkap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing\n",
    "Silakan isi bagian ini dengan script untuk keperluan testing model terbaik yang telah dihasilkan dengan memberikan input berupa data teks yang sama sekali baru dan belum ada di dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
